# -*- coding: utf-8 -*-
"""test_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CBPPCyqiKb-TO4ds2V4ktuykQwSwQ6Ea
"""

import pandas as pd

import numpy as np

import tensorflow as tf

from keras.models import Sequential

from sklearn.metrics import accuracy_score, classification_report

from keras.preprocessing.text import Tokenizer

from tensorflow.keras.preprocessing.sequence import pad_sequences

from keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Dropout,Concatenate, Dot, Reshape

from keras.models import Model

from keras.layers import Conv1D

from keras.layers import MaxPooling1D

from keras.layers import Flatten

from keras.optimizers import Adam

from sklearn.model_selection import train_test_split

data = pd.read_csv("/content/train_headlines.csv")

optimizer = Adam(learning_rate=0.001)

tokenizer = Tokenizer()

tokenizer.fit_on_texts(data['Headline'])

X = tokenizer.texts_to_sequences(data['Headline'])

X = pad_sequences(X, padding='post')

Y = pd.get_dummies(data['Labels'], prefix='bias').values

X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)

input_layer = Input(shape=(X.shape[1],))

embedding_layer = Embedding(len(tokenizer.word_index) + 1, 128)(input_layer)

lstm_layer = Bidirectional(LSTM(128, return_sequences=True))(embedding_layer)

dropout_layer = Dropout(0.5)(lstm_layer)

attention_layer = Dense(1, activation='tanh')(dropout_layer)

attention_weights = tf.nn.softmax(attention_layer, axis=1)

context_vector = Dot(axes=1)([dropout_layer, attention_weights])

output_layer = Dense(Y.shape[1], activation='softmax')(context_vector)

model = Model(inputs=input_layer, outputs=output_layer)

model = Sequential()

model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(22,1)))

model.add(MaxPooling1D(pool_size=2))

model.add(Flatten())

model.add(Dense(128, activation='relu'))

model.add(Dense(3, activation='softmax'))

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

for layer in model.layers:
    print(layer.output_shape)

assert len(X_train) == len(Y_train)

assert len(X_val) == len(Y_val)

model.compile(optimizer=optimizer, loss='binary_crossentropy')

tf.keras.backend.clear_session()

model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=10, batch_size=64)

avg_loss = model.evaluate(X_train, Y_train)
print("Average Loss: ", avg_loss)

accuracy_score = 1 - avg_loss
print('Training Accuracy:', accuracy_score)

new_data = pd.DataFrame({'Headline': ['New headline'], 'Labels': ['Fake']})

new_X = tokenizer.texts_to_sequences(new_data['Headline'])

new_X = pad_sequences(new_X, padding='post', maxlen=X.shape[1])

new_Y = model.predict(new_X)
print('Predicted bias:', new_Y)

new_Y = [[0.3760072,  0.44260344, 0.18138938]]
class_labels = ["Fake\t" , "Real\t", "Satire"]
formatted_output = "Label \t\t\t Probability \n"
for i in range(len(new_Y[0])):
    formatted_output += "{}  \t\t {:.2f}% \n".format(class_labels[i], new_Y[0][i]*100)
print(formatted_output)