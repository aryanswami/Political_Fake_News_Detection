# -*- coding: utf-8 -*-
"""train_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-SvI4-m3Zea3LoBqdQVRi3jrKwGnuPb4
"""

import numpy as np

import pandas as pd

from keras.layers import Input, Embedding, LSTM, Dense, Dropout, Bidirectional, Concatenate, Attention

from keras.layers import Conv1D

from keras.models import Sequential

pip install keras-self-attention

from keras.utils import to_categorical

from keras_self_attention import SeqSelfAttention

from keras.models import Model

from keras.callbacks import EarlyStopping

from keras.preprocessing.text import Tokenizer

from tensorflow.keras.preprocessing.sequence import pad_sequences

from keras.initializers import GlorotNormal

# Load the dataset
data = pd.read_csv("/content/test_headlines_unlabelled.csv")

# Separate the headlines and labels
Headline = data.iloc[:, 0]
labels = data.iloc[:, 1]

# Convert the labels to binary (0 for unbiased, 1 for biased)
labels = np.where(labels == 'unbiased', 0, 1)

# Tokenize the headlines
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(Headline)
sequences = tokenizer.texts_to_sequences(Headline)

# Pad the sequences
max_len = 30
padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')

initializer = GlorotNormal(seed=42)

# Define the model architecture
inputs = Input(shape=(max_len,))
x = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length= max_len)(inputs)
x = Bidirectional(LSTM(64, return_sequences=True))(x)
x = SeqSelfAttention(attention_activation='sigmoid')(x)
x = Dense(64, activation='relu')(x)
x = Dropout(0.2)(x)
outputs = Dense(1, activation='sigmoid')(x)

model = Model(inputs=inputs, outputs=outputs)

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

for layer in model.layers:
    print(layer.output_shape)

# assume logits has shape (None, 30, 1) and labels has shape (None,)
logits = np.random.rand(380)
labels = np.random.randint(0, 2, size=(380,30))

print(padded_sequences.shape)
print(labels.shape)

# Train the model
early_stop = EarlyStopping(monitor='val_loss', patience=2, verbose=1)
model.fit(padded_sequences, labels, batch_size=32, epochs=10, validation_split=0.2) #callbacks=[early_stop])

# Evaluate the model
loss, accuracy = model.evaluate(padded_sequences, labels)
print('Accuracy: {:.2f}%'.format(accuracy * 100))

# Predict on new headlines
new_headlines = ['Breaking News: Trump wins re-election', 'Biden announces new immigration policies']
new_sequences = tokenizer.texts_to_sequences(new_headlines)
new_padded_sequences = pad_sequences(new_sequences, maxlen=max_len, padding='post')
predictions = model.predict(new_padded_sequences)
labels_predicted = np.where(predictions > 0.5, 'biased', 'unbiased')
print('Predictions:', labels_predicted)